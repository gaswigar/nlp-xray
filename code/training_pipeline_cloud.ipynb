{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4cf47ee-334d-46f1-80db-6983245a33bd",
   "metadata": {},
   "source": [
    "### Training Pipeline Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f22040b4-d928-4516-8b3a-d09bd9402b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    " \n",
    "REGION = 'us-central1'\n",
    "PROJECT = !(gcloud config get-value core/project)\n",
    "PROJECT = PROJECT[0]\n",
    "BUCKET =  'nlp-xray-dataset'\n",
    "\n",
    "# # Do not change these\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f302ce85-2f13-4d95-8089-9fedc8a88bc6",
   "metadata": {},
   "source": [
    "### Task setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2220d6f9-91e6-4738-921d-d22d94e90641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting xray_models/trainer_cloud/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile xray_models/trainer_cloud/task.py\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "\n",
    "from . import model\n",
    "\n",
    "def _parse_arguments(argv):\n",
    "    \"\"\"\n",
    "    Parse command line arguments\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--model_type',\n",
    "        help='which model type to use',\n",
    "        type=str, default='mobilenetv2')\n",
    "    parser.add_argument(\n",
    "        '--epoch',\n",
    "        help='number of epochs to use',\n",
    "        type=int, default=5)\n",
    "    parser.add_argument(\n",
    "        '--steps_per_epoch',\n",
    "        help='number of steps per epoch to use',\n",
    "        type=int, default=10)\n",
    "    parser.add_argument(\n",
    "        '--job_dir',\n",
    "        help='directory where to save the model',\n",
    "        type=str, default='xray_models/')\n",
    "    parser.add_argument(\n",
    "        '--nrows',\n",
    "        help='number of total rows desired accross test, validation, and test set',\n",
    "        type=int, default=None)\n",
    "    parser.add_argument('--loss_class_weighted', dest='loss_class_weighted', action='store_true')\n",
    "    parser.add_argument('--loss_not_class_weighted', dest='loss_class_weighted', action='store_false')\n",
    "    parser.set_defaults(loss_class_weighted=True)\n",
    "    return parser.parse_known_args(argv)\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    This function will parse command line arguments and kick model training\n",
    "    \"\"\"\n",
    "    args = _parse_arguments(sys.argv[1:])[0]\n",
    "    print(f'Training with Arguments:{args}')\n",
    "    trial_id = json.loads(\n",
    "        os.environ.get('TF_CONFIG', '{}')).get('task', {}).get('trial', '')\n",
    "    output_path = args.job_dir if not trial_id else args.job_dir+'/'\n",
    "    print(args.job_dir)\n",
    "    model_layers = model.get_layers(args.model_type)\n",
    "    model_history = model.train_and_evaluate(model_layers,args.epoch, args.steps_per_epoch, args.job_dir, args.nrows, args.loss_class_weighted, model_type=args.model_type)\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01f4c8b-58ef-46ba-91c3-735cbdc0d053",
   "metadata": {},
   "source": [
    "### Adding non model functions to a config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd1baee7-6a57-4da5-8578-a449f9aa17a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting xray_models/trainer_cloud/config/config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile xray_models/trainer_cloud/config/config.py\n",
    "\n",
    "#model parameters\n",
    "IMG_HEIGHT = 299\n",
    "IMG_WIDTH = 299\n",
    "IMG_CHANNELS = 1\n",
    "\n",
    "processing_parameters={'cnn': {'channels': 1, 'image_height': min([IMG_HEIGHT,299]), 'image_width': min([IMG_WIDTH,299]), 'pixel_scale_min': 0, 'pixel_scale_max': 1}, \n",
    "                       'inception_resnet': {'channels': 3, 'image_height': 224, 'image_width': 224, 'pixel_scale_min': 0, 'pixel_scale_max': 1}, \n",
    "                       'vision_transformer': {'channels': 3, 'image_height': 224, 'image_width': 224, 'pixel_scale_min': -1, 'pixel_scale_max': 1}}\n",
    "BATCH_SIZE = 15\n",
    "SHUFFLE_BUFFER = 20*BATCH_SIZE\n",
    "VAL_BATCH_SIZE = 10\n",
    "\n",
    "#augmentation parameters\n",
    "MAX_DELTA = 63.0/255.0 #adjusting brightness\n",
    "CONTRAST_LOWER = 0.2\n",
    "CONTRAST_UPPER = 1.8\n",
    "\n",
    "#response variable columns in the mete_data_processed.csv\n",
    "response_variables = ['atelectasis', 'cardiomegaly', 'consolidation', 'edema', 'effusion', 'emphysema', 'fibrosis', 'hernia',\n",
    "                 'infiltration', 'mass', 'no_finding', 'nodule', 'pleural_thickening', 'pneumonia', 'pneumothorax']\n",
    "\n",
    "meta_data_train_processed_path = \"gs://nlp-xray-dataset/meta_data/meta_data_processed_train.csv\" #location to the meta data file - train \n",
    "meta_data_val_processed_path = \"gs://nlp-xray-dataset/meta_data/meta_data_processed_val.csv\" #location to the meta data file - val\n",
    "meta_data_test_processed_path = \"gs://nlp-xray-dataset/meta_data/meta_data_processed_test.csv\" #location to the meta data file - test\n",
    "meta_data_processed_path = \"gs://nlp-xray-dataset/meta_data/meta_data_processed.csv\" #location to the meta data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8750cd6-b1d7-46a5-8783-a1397cfd2369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting xray_models/trainer_cloud/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile xray_models/trainer_cloud/preprocessing.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from .config import config\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "def rescale_image_tensor(t,domain_interval,range_interval):\n",
    "    a=domain_interval[0]\n",
    "    b=domain_interval[1]\n",
    "    c=range_interval[0]\n",
    "    d=range_interval[1]\n",
    "    rescaled_t=c+((d-c)/(b-a))*(t-a)\n",
    "    return(rescaled_t)\n",
    "\n",
    "def decode_image(img, reshape_dims, num_channels, pixel_min, pixel_max):\n",
    "    \"\"\"\n",
    "    Decode an image\n",
    "    \"\"\"\n",
    "    img = tf.image.decode_png(img, channels=num_channels)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.resize(img, reshape_dims)\n",
    "    image_min = tf.math.reduce_min(img)\n",
    "    image_max = tf.math.reduce_max(img)    \n",
    "    if pixel_min!=image_min and pixel_max!=image_max:\n",
    "        #img=img.map(lambda x: tf.py_function(func=rescale_image_tensor, inp=[x, [image_min,image_max],[pixel_min,pixel_max]], Tout=tf.float32))\n",
    "        #img = img.map(lambda x: rescale_image_tensor(x,[image_min,image_max],[pixel_min,pixel_max]))\n",
    "        img = rescale_image_tensor(img, [image_min,image_max],[pixel_min,pixel_max])\n",
    "    img = tf.cast(img, dtype=tf.float32)\n",
    "    return(img)\n",
    "\n",
    "def decode(filename, label):\n",
    "    \"\"\"\n",
    "    Decode file names.\n",
    "    \"\"\"\n",
    "    image_bytes = tf.io.read_file(filename=filename)\n",
    "    return image_bytes, label\n",
    "\n",
    "\n",
    "def get_filenames_and_labels(data_path,nrow_ind):\n",
    "    \"\"\"\n",
    "    Get filenames and labels\n",
    "    \"\"\"\n",
    "    meta_data = pd.read_csv(data_path)\n",
    "    all_files = meta_data['gs_path'].tolist()\n",
    "    response_vars = config.response_variables    \n",
    "    all_labels = np.array(meta_data[response_vars].values.tolist())\n",
    "    if nrow_ind:\n",
    "        if nrow_ind<= meta_data.shape[0]:            \n",
    "            zip_lists=list(zip(all_files, all_labels))\n",
    "            random.shuffle(zip_lists)\n",
    "            files, labels = zip(*zip_lists)\n",
    "            files=list(files)[:nrow_ind]\n",
    "            labels=np.array(labels)[:nrow_ind]\n",
    "            return(files, labels)\n",
    "        else:\n",
    "            return(all_files, all_labels)\n",
    "    else: \n",
    "        return(all_files, all_labels)\n",
    "\n",
    "        \n",
    "\n",
    "def read_and_preprocess(image_bytes, label,model_type, random_augment=False):\n",
    "    \"\"\"\n",
    "    Function which performs data augmentation.\n",
    "    \"\"\"\n",
    "    pp=config.processing_parameters[model_type]\n",
    "    if random_augment:\n",
    "        img = decode_image(image_bytes, [pp['image_width']+10,pp['image_width']+10], pp['channels'], pp['pixel_scale_min'], pp['pixel_scale_max'])\n",
    "        img = tf.image.random_crop(img, [pp['image_width'],pp['image_width'], pp['channels']])\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        img = tf.image.random_brightness(img, config.MAX_DELTA)\n",
    "        img = tf.image.random_contrast(img, config.CONTRAST_LOWER, config.CONTRAST_UPPER)\n",
    "    else:\n",
    "        img = decode_image(image_bytes, [pp['image_width'],pp['image_width']], pp['channels'], pp['pixel_scale_min'], pp['pixel_scale_max'])\n",
    "    return img, label\n",
    "\n",
    "def read_and_preprocess_with_augment(image_bytes, label, model_type):\n",
    "    \"\"\"\n",
    "    Data augmentation for the training set.\n",
    "    \"\"\"\n",
    "    return read_and_preprocess(image_bytes, label, model_type, random_augment=True)\n",
    "\n",
    "\n",
    "def load_dataset(filenames, labels, model_type, batch_size=None, training=True):\n",
    "    \"\"\"\n",
    "    This functions load the dataset from the GCS bucket.\n",
    "    Inputs include:\n",
    "    filenames: list of gcs locations for image files\n",
    "    labels: numpy array of one hot encoded multi labels\n",
    "    batch_size: batch size\n",
    "    training: boolean entry specifying if training data is needed. False for test data.\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels)).map(decode) #numpy array of filenames and numpy array of labels    \n",
    "    if training:\n",
    "        dataset = dataset.map(lambda f,l: read_and_preprocess_with_augment(f,l,model_type=model_type)).cache().shuffle(config.SHUFFLE_BUFFER).repeat(count=None)\n",
    "    else:\n",
    "        dataset = dataset.map(lambda f,l: read_and_preprocess(f,l,model_type=model_type)).repeat(count=1)\n",
    "    return dataset.batch(batch_size=batch_size).prefetch(buffer_size=AUTOTUNE) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf5316c-ccb2-4cdb-9ec8-85c8eed2d8e0",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e7407eb-aff4-40a2-9af6-b4b34209c263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting xray_models/trainer_cloud/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile xray_models/trainer_cloud/model.py\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import (Conv2D, Dense, Dropout, Flatten, MaxPooling2D, Softmax)\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from .config import config\n",
    "from .import preprocessing\n",
    "\n",
    "def get_layers(model_type, \n",
    "               nclasses=15, \n",
    "               hidden_layer_1_neurons=400,\n",
    "               hidden_layer_2_neurons=100,\n",
    "               dropout_rate=0.25,\n",
    "               num_filters_1=64,\n",
    "               kernel_size_1=3,\n",
    "               pooling_size_1=2,\n",
    "               num_filters_2=32,\n",
    "               kernel_size_2=3,\n",
    "               pooling_size_2=2):\n",
    "    \"\"\"\n",
    "    Get model layers for a specific model\n",
    "    \"\"\"\n",
    "    model_layers = {\n",
    "        'cnn':[\n",
    "            Conv2D(num_filters_1, kernel_size=kernel_size_1,\n",
    "                  activation='relu', input_shape=(config.IMG_WIDTH, config.IMG_HEIGHT, 1)),\n",
    "            MaxPooling2D(pooling_size_1),\n",
    "            Conv2D(num_filters_2, kernel_size=kernel_size_2,\n",
    "                  activation='relu'),\n",
    "            MaxPooling2D(pooling_size_2),\n",
    "            Flatten(),\n",
    "            Dense(hidden_layer_1_neurons, activation='relu'),\n",
    "            Dense(hidden_layer_2_neurons, activation='relu'),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(nclasses, activation='sigmoid')\n",
    "        ],\n",
    "        'vision_transformer':\n",
    "        [hub.KerasLayer(\"https://tfhub.dev/sayakpaul/vit_b16_fe/1\", trainable=False),\n",
    "        Dense(nclasses, activation='sigmoid')\n",
    "        ],\n",
    "        'inception_resnet':[\n",
    "        hub.KerasLayer(\"https://tfhub.dev/google/imagenet/resnet_v2_101/feature_vector/5\",trainable=False),\n",
    "        Dense(nclasses, activation='sigmoid')]\n",
    "        \n",
    "    }\n",
    "    return model_layers[model_type]\n",
    "\n",
    "def label_weighted_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Loss \n",
    "    \"\"\"\n",
    "    P = tf.reduce_sum(y_true)\n",
    "    N = -1 * tf.reduce_sum(y_true - 1)\n",
    "    \n",
    "    beta_P = tf.cast((P + N) / P, dtype=tf.float64)\n",
    "    beta_N = tf.cast((P + N) / N, dtype=tf.float64)\n",
    "    \n",
    "    y_true = tf.cast(y_true, dtype=tf.float64)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float64)\n",
    "    \n",
    "    epsilon = tf.constant(1e-7, dtype=tf.float64) #avoid nans\n",
    "    loss = (beta_P*tf.math.log(y_pred+epsilon)*y_true + beta_N*tf.math.log((1-y_pred)+epsilon) * (1-y_true))*-1.0\n",
    "    tf.debugging.assert_all_finite(loss, 'There are nan values')\n",
    "    return tf.reduce_sum(tf.reduce_mean(loss, axis = 0))\n",
    "\n",
    "\n",
    "class ClassImbalanceSparsityAdjustedLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, inverse_class_weights):\n",
    "        \"\"\"\n",
    "        Initialization of inverse class weights\n",
    "        \"\"\"\n",
    "        super().__init__(name = 'ClassImbalanceSparsityAdjustedLoss')\n",
    "        self.inverse_class_weights = inverse_class_weights\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Cross entropy loss adjusted for class imabalance and one-hot encoding sparsity\n",
    "        \"\"\"\n",
    "        P = tf.reduce_sum(y_true)\n",
    "        N = -1 * tf.reduce_sum(y_true - 1)\n",
    "\n",
    "        beta_P = tf.cast((P + N) / P, dtype=tf.float64)\n",
    "        beta_N = tf.cast((P + N) / N, dtype=tf.float64)\n",
    "\n",
    "        y_true = tf.cast(y_true, dtype=tf.float64)\n",
    "        y_pred = tf.cast(y_pred, dtype=tf.float64)\n",
    "\n",
    "        epsilon = tf.constant(1e-7, dtype=tf.float64) #avoid nans\n",
    "        loss = (beta_P*tf.math.log(y_pred+epsilon)*y_true + beta_N*tf.math.log((1-y_pred)+epsilon) * (1-y_true))*-1.0\n",
    "        tf.debugging.assert_all_finite(loss, 'There are nan values')\n",
    "        return tf.reduce_sum(tf.reduce_mean(loss, axis = 0)*self.inverse_class_weights) \n",
    "\n",
    "\n",
    "def build_model(layers, output_dir,inverse_class_weights, loss_class_weighted, model_type):\n",
    "    \"\"\"\n",
    "    Compiles keras model for image classification/\n",
    "    \"\"\"    \n",
    "\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    \n",
    "    with strategy.scope():\n",
    "        recall = tf.keras.metrics.Recall()\n",
    "        precision = tf.keras.metrics.Precision()\n",
    "        #original_loss_func - label_weighted_cross_entropy\n",
    "        \n",
    "        model = Sequential(layers)\n",
    "        # 'vision_transformer',\n",
    "        if model_type in ['inception_resnet']:\n",
    "            model.build([None, 224, 224, 3]) \n",
    "\n",
    "    if loss_class_weighted: \n",
    "        model.compile(optimizer='adam',\n",
    "                     loss=ClassImbalanceSparsityAdjustedLoss(inverse_class_weights),\n",
    "                     metrics=[recall, precision, 'accuracy'])\n",
    "    else: \n",
    "        model.compile(optimizer='adam',\n",
    "                     loss=label_weighted_cross_entropy,\n",
    "                     metrics=[recall, precision, 'accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(model_layers,num_epochs, steps_per_epoch, output_dir, nrows, loss_class_weighted, model_type):\n",
    "    \"\"\"\n",
    "    Compile the model and load data for training into it.\n",
    "    \"\"\"\n",
    "    if nrows: \n",
    "        train_nrow=int(nrows*.8)\n",
    "        val_nrow=int(nrows*.2)\n",
    "    else: \n",
    "        train_nrow=None\n",
    "        val_nrow=None\n",
    "    \n",
    "    # Training Dataset \n",
    "    train_filenames, train_labels  = preprocessing.get_filenames_and_labels(config.meta_data_train_processed_path, \n",
    "                                                                            nrow_ind=train_nrow)\n",
    "    # CALCULATE WEIGHTS HERE \n",
    "    labels_df = pd.DataFrame(train_labels,columns=config.response_variables)\n",
    "    total = labels_df.sum(axis=0).sum()\n",
    "    inverse_props = 1/labels_df.sum(axis=0).div(total)\n",
    "    normalized_inverse_props = inverse_props.div(inverse_props.sum())\n",
    "    inverse_class_weights=np.array(normalized_inverse_props)\n",
    "    \n",
    "    # Validation Dataset\n",
    "    val_filenames, val_labels  = preprocessing.get_filenames_and_labels(config.meta_data_val_processed_path,\n",
    "                                                                            nrow_ind=val_nrow)\n",
    "    \n",
    "    model = build_model(model_layers, output_dir=output_dir, inverse_class_weights=inverse_class_weights, loss_class_weighted=loss_class_weighted, model_type=model_type)\n",
    "\n",
    "    print(f'Number of Training Records:{len(train_filenames)}')\n",
    "    print(f'Number of Validation Records:{len(val_filenames)}')\n",
    "    \n",
    "    #create training and validation data\n",
    "    print(model_type)\n",
    "    train_ds = preprocessing.load_dataset(train_filenames, train_labels, model_type=model_type,batch_size=config.BATCH_SIZE)\n",
    "    if not config.VAL_BATCH_SIZE:\n",
    "        val_batch_size=val_filenames.shape[0]\n",
    "    else: \n",
    "        val_batch_size=config.VAL_BATCH_SIZE\n",
    "        \n",
    "    val_ds = preprocessing.load_dataset(val_filenames, val_labels, model_type=model_type,batch_size=val_batch_size,training=False)\n",
    "    \n",
    "    callbacks = []\n",
    "    if output_dir:\n",
    "        tensorboard_callback = TensorBoard(log_dir=output_dir)\n",
    "        callbacks = [tensorboard_callback]\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks)\n",
    "    \n",
    "    if output_dir:\n",
    "        export_path = os.path.join(output_dir, 'keras_export')\n",
    "        model.save(export_path, save_format='tf')\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f872ba-b5b0-498a-ae08-ca45955356c9",
   "metadata": {},
   "source": [
    "# Running Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de72386d-e201-48d1-8ebf-e4206ef91276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_type = 'vision_transformer'\n",
    "#model_type = 'vision_transformer'\n",
    "model_type = 'cnn'\n",
    "os.environ[\"MODEL_TYPE\"] = model_type\n",
    "job_dir = f\"xray_models/models/{model_type}_{current_time}/\"\n",
    "os.environ[\"JOB_DIR\"] = job_dir\n",
    "os.environ['JOB_FOLDER_NAME']=f'{model_type}_{current_time}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bed80c66-488c-4c68-a9fe-45fc68aa8089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xray_models/models/cnn_20211115_170342/\n"
     ]
    }
   ],
   "source": [
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(f\"xray_models/models/{model_type}_{current_time}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "244ddd12-aaff-4bef-9e3d-6580b42882dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python3 -m xray_models.trainer_cloud.task     --job_dir=xray_models/models/cnn_20211115_170342/     --epochs=5     --steps_per_epoch=6000     --model_type=cnn     --loss_class_weighted'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "f\"\"\"python3 -m xray_models.trainer_cloud.task \\\n",
    "    --job_dir={job_dir} \\\n",
    "    --epochs=5 \\\n",
    "    --steps_per_epoch=6000 \\\n",
    "    --model_type={model_type} \\\n",
    "    --loss_class_weighted\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7206667a-dd27-40fa-ae3b-0fa659a465d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m84",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m84"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
